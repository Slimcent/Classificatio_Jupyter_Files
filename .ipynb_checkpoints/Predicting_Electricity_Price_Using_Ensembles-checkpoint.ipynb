{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad22fd1",
   "metadata": {},
   "source": [
    "Ensemble models combine predictions from multiple individual models to improve overall performance and robustness. \n",
    "They leverage the strengths of various algorithms to produce a final prediction that is often more accurate and less prone to overfitting than any single model alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35034b7c",
   "metadata": {},
   "source": [
    "There are several types of ensemble methods, which are;\n",
    "\n",
    "Bagging (Bootstrap Aggregating)\n",
    "How it Works: Multiple copies of the same algorithm are trained on different subsets of the training data (created by bootstrapping, which involves sampling with replacement). The final prediction is made by aggregating the predictions from all models, typically by averaging for regression tasks and majority voting for classification tasks.\n",
    "Example: Random Forest is a popular bagging algorithm that uses decision trees as the base models.\n",
    "\n",
    "Boosting\n",
    "How it Works: Models are trained sequentially, where each new model focuses on the errors made by the previous models. The predictions from all models are combined, often with weights assigned to each model based on its performance.\n",
    "Example: AdaBoost and Gradient Boosting Machines (GBM) are common boosting techniques.\n",
    "\n",
    "Stacking (Stacked Generalization)\n",
    "How it Works: Multiple models (base learners) are trained on the same dataset. A meta-model (or second-level model) is then trained on the outputs of the base models to make the final prediction. This meta-model learns how to best combine the predictions of the base models.\n",
    "Example: You might use logistic regression as a meta-model to combine the outputs of several different base classifiers.\n",
    "\n",
    "Voting\n",
    "How it Works: Multiple models (typically of different types) are trained, and their predictions are combined through a voting scheme. For classification tasks, it could be majority voting or weighted voting, and for regression, it could be averaging.\n",
    "Example: A simple voting classifier that uses the majority vote of decision trees, support vector machines, and k-nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804b1c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
